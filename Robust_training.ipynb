{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "FKZDMi_5iKCA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.colors import LightSource\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "e-A0EQP5iN5L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# download MNIST training and testing datasets, then prepare corresponding dataloaders (batch size = 100)\n",
        "mnist_train = datasets.MNIST(\"../data\", train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_test = datasets.MNIST(\"../data\", train=False, download=True, transform=transforms.ToTensor())\n",
        "train_loader = DataLoader(mnist_train, batch_size = 100, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size = 100, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PjUrtvq_iUbQ"
      },
      "outputs": [],
      "source": [
        "# initialize the CNN architecture with 4 convolutional layers and 2 MLP layers for standard training\n",
        "torch.manual_seed(0)\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.shape[0], -1)\n",
        "\n",
        "model_cnn = nn.Sequential(nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
        "                          nn.Conv2d(32, 32, 3, padding=1, stride=2), nn.ReLU(),\n",
        "                          nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
        "                          nn.Conv2d(64, 64, 3, padding=1, stride=2), nn.ReLU(),\n",
        "                          Flatten(),\n",
        "                          nn.Linear(7*7*64, 100), nn.ReLU(),\n",
        "                          nn.Linear(100, 10)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Your task: complete the following function\n",
        "\n",
        "\n",
        "\n",
        "def pgd(model, X, y, epsilon=0.1, alpha=0.02, num_iter=10):\n",
        "    \"\"\"Construct adversarial examples using PGD with TRADES\"\"\"\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    delta = torch.zeros_like(X, requires_grad=True)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    pert = X.clone().detach()\n",
        "    pert = pert.to(device)\n",
        "\n",
        "    for _ in range(num_iter):\n",
        "        output = model(pert + delta)\n",
        "        loss =criterion(output, y)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        delta.data = (delta + alpha * delta.grad.detach().sign()).clamp(-epsilon, epsilon)\n",
        "        delta.grad.zero_()\n",
        "\n",
        "    perturbed_X = torch.clamp(pert + delta, 0, 1)\n",
        "    return perturbed_X"
      ],
      "metadata": {
        "id": "G8QZDPcYVaQL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Your task: complete the following functions\n",
        "\n",
        "def epoch(loader, model, opt=None):\n",
        "    \"\"\"Standard training/evaluation epoch over the dataset\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    running_loss = 0.0\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "\n",
        "    if opt:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        if opt:\n",
        "            opt.zero_grad()\n",
        "\n",
        "        inputs.requires_grad_()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        if opt:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n"
      ],
      "metadata": {
        "id": "4oHnRQ5aW_TP"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_adv(loader, model, attack, opt=None, **kwargs):\n",
        "    \"\"\"Adversarial training/evaluation epoch over the dataset\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    running_loss = 0.0\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "\n",
        "    if opt:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        adv_inputs = attack(model, inputs, labels, **kwargs)\n",
        "\n",
        "        if opt:\n",
        "            opt.zero_grad()\n",
        "\n",
        "        adv_inputs.requires_grad_()\n",
        "        outputs = model(adv_inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        if opt:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n"
      ],
      "metadata": {
        "id": "_4vstnF5f9JH"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDwITEWRiaxv",
        "outputId": "8b90a64e-0fef-4373-fad9-3f81c847dd55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.000032\t0.050911\t3.712060\n",
            "0.000029\t0.051287\t3.798647\n",
            "0.000026\t0.051819\t3.795244\n",
            "0.000023\t0.052474\t3.814792\n",
            "0.000022\t0.052841\t3.860831\n"
          ]
        }
      ],
      "source": [
        "# specify the optimizer as SGD\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# specify the optimizer as SGD\n",
        "opt = optim.SGD(model_cnn.parameters(), lr=1e-1)\n",
        "\n",
        "# standard training\n",
        "for t in range(5):\n",
        "    train_err, train_loss = epoch(train_loader, model_cnn, opt)\n",
        "    test_err, test_loss = epoch(test_loader, model_cnn)\n",
        "    adv_err, adv_loss = epoch_adv(test_loader, model_cnn, pgd)\n",
        "\n",
        "    print(*(\"{:.6f}\".format(i) for i in (train_err, test_err, adv_err)), sep=\"\\t\")\n",
        "\n",
        "# save the standard trained model for further evaluation\n",
        "torch.save(model_cnn.state_dict(), \"model_cnn.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the results of the standard training we can see how senstive the model attacks. The output suggests that we have very high adverserial error, which is expected, since the model is not trained on adverserial examples."
      ],
      "metadata": {
        "id": "2dz6NEKeifG1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "4Dht-vSxlloO"
      },
      "outputs": [],
      "source": [
        "# use the same CNN architecture for robust training\n",
        "model_cnn_robust = nn.Sequential(nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
        "                                 nn.Conv2d(32, 32, 3, padding=1, stride=2), nn.ReLU(),\n",
        "                                 nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
        "                                 nn.Conv2d(64, 64, 3, padding=1, stride=2), nn.ReLU(),\n",
        "                                 Flatten(),\n",
        "                                 nn.Linear(7*7*64, 100), nn.ReLU(),\n",
        "                                 nn.Linear(100, 10)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "3KKsmaBalooX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e376afdf-97df-4824-e7c4-2e923061ebae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.583447\t0.147872\t0.332344\n",
            "0.232554\t0.060061\t0.155951\n",
            "0.149133\t0.048194\t0.138092\n",
            "0.116772\t0.037064\t0.110379\n",
            "0.097984\t0.033818\t0.101471\n"
          ]
        }
      ],
      "source": [
        "# specify the optimizer as SGD\n",
        "opt = optim.SGD(model_cnn_robust.parameters(), lr=1e-1)\n",
        "\n",
        "# PGD-based adversarial training\n",
        "for t in range(5):\n",
        "    train_err, train_loss = epoch_adv(train_loader, model_cnn_robust, pgd, opt)\n",
        "    test_err, test_loss = epoch(test_loader, model_cnn_robust)\n",
        "    adv_err, adv_loss = epoch_adv(test_loader, model_cnn_robust, pgd)\n",
        "\n",
        "    print(*(\"{:.6f}\".format(i) for i in (train_err, test_err, adv_err)), sep=\"\\t\")\n",
        "\n",
        "# save the standard trained model for further evaluation\n",
        "torch.save(model_cnn_robust.state_dict(), \"model_cnn_robust.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see that the output of adverserial error is decreased since model got more robust after training with adverserial examples. However, we can see that there is a difference in the outputs test error for standard training. There is slight increase in them, suggesting there might be a trade of between robustness and accuracy. But with more iteration that difference also disappears.\n"
      ],
      "metadata": {
        "id": "iLuF-XhEjoRY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "fT5MZujiN9Da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "651b7923-32c2-4007-c38d-3cc85e21306d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# load the standard trained and adversarially trained models\n",
        "model_cnn.load_state_dict(torch.load(\"model_cnn.pt\"))\n",
        "model_cnn_robust.load_state_dict(torch.load(\"model_cnn_robust.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "f48ELj_UN-uI"
      },
      "outputs": [],
      "source": [
        "def fgsm(model, X, y, epsilon=0.1):\n",
        "    \"\"\" Construct FGSM adversarial examples for the example (X,y)\"\"\"\n",
        "    delta = torch.zeros_like(X, requires_grad=True)\n",
        "    loss = nn.CrossEntropyLoss()(model(X + delta), y)\n",
        "    loss.backward()\n",
        "    return epsilon * delta.grad.detach().sign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ecDX3ziXPibw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb026e27-c750-492b-f1ea-d86194b8e1b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clean: 0.0528 0.0338\n",
            "FGSM:  6.2064 2.3046\n",
            "PGD (10 iter): 3.8608 0.1015\n"
          ]
        }
      ],
      "source": [
        "# clean performance (no attack)\n",
        "print(\"clean:\", \"{:.4f}\".format(epoch(test_loader, model_cnn)[0]),\n",
        "      \"{:.4f}\".format(epoch(test_loader, model_cnn_robust)[0]))\n",
        "\n",
        "# evaluate both models using FGSM attack\n",
        "print(\"FGSM: \", \"{:.4f}\".format(epoch_adv(test_loader, model_cnn, fgsm)[0]),\n",
        "      \"{:.4f}\".format(epoch_adv(test_loader, model_cnn_robust, fgsm)[0]))\n",
        "\n",
        "# evaluate both models using PGD attack\n",
        "print(\"PGD (10 iter):\", \"{:.4f}\".format(epoch_adv(test_loader, model_cnn, pgd, num_iter=10)[0]),\n",
        "      \"{:.4f}\".format(epoch_adv(test_loader, model_cnn_robust, pgd, num_iter=10)[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation of the models using two different attacks show, that the robust model is less sensitive to pgd attack but still shows high sensitivity to the FGSM attack. This suggests that the model is vulnerable against small perturbations used by FSGM Attack. However, we can also see that the error decreases for the model that was trained on adverserial examples."
      ],
      "metadata": {
        "id": "pze7yeTlltTh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EfxdKahQL3v"
      },
      "outputs": [],
      "source": [
        "#### Your task (bonus): develop an attack method to achieve an attack success rate as high as possible. You can modify the following function if needed.\n",
        "\n",
        "# You can try out some of the attack methods introduced in Lectures 3-4 or develop your unique creative attack.\n",
        "# In principle, the performance of your attack should be better than FGSM or PGD, 10 iter;\n",
        "# The higher attack success rates you can achieve, the higher credits you may receive.\n",
        "\n",
        "def my_attack(model, X, y, epsilon=0.1):\n",
        "  \"\"\" Construct adversarial examples for the example (X,y)\"\"\"\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQVHww4GRwn6"
      },
      "outputs": [],
      "source": [
        "print(\"My Attack: \", \"{:.4f}\".format(epoch_adv(test_loader, model_cnn, my_attack)[0]),\n",
        "      \"{:.4f}\".format(epoch_adv(test_loader, model_cnn_robust, my_attack)[0]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}